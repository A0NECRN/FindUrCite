import os
import sys
import argparse
from searcher import Searcher
from analyzer import Analyzer
from code_finder import CodeFinder
import time

def generate_report(user_input, results, filename="research_result.md"):
    with open(filename, "w", encoding="utf-8") as f:
        f.write(f"# Research Report: {user_input}\n\n")
        
        # Disclaimer
        f.write("> **Disclaimer**: This report is generated by an AI system (FindUrCite). "
                "The analysis (problem definition, methodology, etc.) is inferred from paper abstracts. "
                "Please verify with the original papers. "
                "Rate limits or network errors may affect result completeness.\n\n")

        # Table Header based on Excel Format (21 columns)
        headers = [
            "序号", "竞品/关键 paper 题目", "年份", "等级\n(1-5，5最优)", "关键词", 
            "发表期刊/会议，等级", "作者信息", "单位信息", "细分领域", "链接", "PDF",
            "解决了什么问题 + 问题数学定义", 
            "解决了什么瓶颈问题？用的什么方法？", 
            "方法关键词", "算法流程\n(建议使用伪代码)", 
            "实验设置\n(数据集, 优越性, 对比方法...)", 
            "缺陷\n(为我们工作入场铺路)", 
            "阅读者评价\n(改进点, 文章缺陷, 复现难度等)", 
            "代码仓库链接",
            "数据集", "其他"
        ]
        
        f.write("| " + " | ".join(headers) + " |\n")
        f.write("| " + " | ".join(["---"] * len(headers)) + " |\n")
        
        for i, item in enumerate(results):
            p = item['paper']
            a = item.get('analysis', {})
            c = item.get('codes', [])
            
            # Format Authors
            authors_str = ", ".join(p.get('authors', [])[:3])
            if len(p.get('authors', [])) > 3:
                authors_str += " et al."
            
            # Institutions (Now fetched from Searcher if available)
            inst_str = "Not available"
            if p.get('affiliations'):
                inst_str = ", ".join(p.get('affiliations')[:2])
            
            # PDF Link
            pdf_link = "None"
            if p.get('openAccessPdf'):
                pdf_link = f"[PDF]({p.get('openAccessPdf', {}).get('url', '#')})"
            
            # Format Code
            code_str = "None"
            if c:
                top_code = c[0]
                code_str = f"[{top_code['repo_name']}]({top_code['url']}) (⭐{top_code['stars']})"

            # Clean text to avoid breaking markdown table
            def clean(text):
                if not isinstance(text, str): return str(text)
                return text.replace("\n", "<br>").replace("|", "\|")

            row = [
                str(i + 1),
                clean(p.get('title', 'N/A')),
                str(p.get('year', 'N/A')),
                str(a.get('relevance_score', 0)),
                clean(", ".join(p.get('keywords', [])[:3]) if p.get('keywords') else "N/A"),
                clean(p.get('venue', 'N/A')), 
                clean(authors_str),
                clean(inst_str),
                clean(a.get('sub_field', 'N/A')),
                f"[Link]({p.get('url', '#')})",
                pdf_link,
                clean(a.get('problem_def', 'N/A')),
                clean(a.get('methodology', 'N/A')),
                clean(a.get('method_keywords', 'N/A')),
                clean(a.get('algorithm_summary', 'N/A')),
                clean(a.get('experiments', 'N/A')),
                clean(a.get('limitations', 'N/A')),
                clean(a.get('critique', 'N/A')),
                code_str,
                clean(a.get('datasets', 'N/A')),
                clean(a.get('others', 'N/A'))
            ]
            
            f.write("| " + " | ".join(row) + " |\n")
    
    print(f"[Main] Report generated: {filename}")

def main():
    parser = argparse.ArgumentParser(description="FindUrCite - AI Research Assistant")
    parser.add_argument("input", help="Your research idea, draft text, or path to a text file (.txt)")
    parser.add_argument("--model", default="qwen2.5:7b", help="Ollama model to use")
    parser.add_argument("--output", default="research_result.md", help="Output filename")
    args = parser.parse_args()

    # Handle file input
    user_text = args.input
    if os.path.exists(args.input) and os.path.isfile(args.input):
        try:
            with open(args.input, 'r', encoding='utf-8') as f:
                user_text = f.read()
            print(f"[Main] Loaded text from file: {args.input} ({len(user_text)} chars)")
        except Exception as e:
            print(f"[Main] Error reading file: {e}")
            return

    print(f"Initializing modules with model: {args.model}...")
    
    try:
        analyzer = Analyzer(model=args.model)
        searcher = Searcher()
        code_finder = CodeFinder()
    except Exception as e:
        print(f"Initialization failed: {e}")
        return

    # Step 1: Analyze User Input (New Logic)
    print("Step 1: Analyzing user input...")
    input_analysis = analyzer.analyze_user_input(user_text)
    
    core_contribution = input_analysis.get('core_contribution', 'N/A')
    search_keywords = input_analysis.get('search_keywords', [])
    key_viewpoint = input_analysis.get('key_viewpoint', user_text[:100])
    
    print(f"  - Core Contribution: {core_contribution}")
    print(f"  - Keywords: {search_keywords}")
    print(f"  - Key Viewpoint: {key_viewpoint}")
    
    # Step 2: Search Papers
    print("Step 2: Searching papers...")
    search_query = " ".join(search_keywords[:4]) # Use top 4 keywords
    print(f"  - Search Query: {search_query}")
    papers = searcher.search_all(search_query, limit_per_source=3)
    
    if not papers:
        print("No papers found.")
        return

    # Step 3: Find Code (Parallel)
    print("Step 3: Finding code (Parallel)...")
    paper_titles = [p['title'] for p in papers]
    code_results = code_finder.find_codes_parallel(paper_titles)

    # Step 4: Analyze Papers (Sequential - constrained by GPU VRAM)
    print("Step 4: Analyzing papers (Deep Read)...")
    final_results = []
    
    for i, paper in enumerate(papers):
        print(f"[{i+1}/{len(papers)}] Analyzing: {paper['title'][:30]}...")
        
        # Analyze against the extracted Key Viewpoint
        analysis = analyzer.analyze_paper_details(key_viewpoint, paper)
        
        final_results.append({
            'paper': paper,
            'analysis': analysis,
            'codes': code_results.get(paper['title'], [])
        })

    # Sort by relevance score
    final_results.sort(key=lambda x: x['analysis'].get('relevance_score', 0), reverse=True)

    # Step 5: Generate Report
    # Pass the Core Contribution as the "User Input" context for the report
    report_context = f"**Draft Analysis:** {core_contribution}\n\n**Viewpoint:** {key_viewpoint}"
    generate_report(report_context, final_results, args.output)

if __name__ == "__main__":
    main()
